
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Preprocessing pipeline &#8212; NeuroMiner Manual</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Classification algorithm" href="3.2.03_paramtemp_classification_algorithm.html" />
    <link rel="prev" title="Cross-validation settings" href="3.2.01_paramtemp_cv_settings.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/nm_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">NeuroMiner Manual</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    <no title>
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1.0_introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.1_prerequisites.html">
   Suggested Prerequisites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.2_gettingstarted.html">
   Getting Started
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  NeuroMiner interface
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="3.0_mainmenu.html">
   Main interface overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.1_mainmenu_input_data.html">
   Data entry in NeuroMiner
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="3.2_mainmenu_define_parameter_template.html">
   Define parameter template
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="3.2.00_paramtemp_data_fusion.html">
     Data Fusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.2.01_paramtemp_cv_settings.html">
     Cross-validation settings
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Preprocessing pipeline
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.2.03_paramtemp_classification_algorithm.html">
     Classification algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.2.04_paramtemp_learning_algorithm_parameters.html">
     Learning algorithm parameters
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.2.05_paramtemp_ensemble_generation_strategies.html">
     Ensemble generation strategies
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.2.06_paramtemp_visualization_options.html">
     Visualization options {#3.2.06_visualization_options}
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.2.07_paramtemp_model_saving_options.html">
     Model saving options
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.2.08_paramtemp_save_parameter_template.html">
     Save parameter template
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.2.09_paramtemp_load_training_template.html">
     Load training template
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.2.10_paramtemp_define_verbosity_level.html">
     Define verbosity level
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.2.11_paramtemp_stacking.html">
     Stacking
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.3_mainmenu_initialize_delete_analyses.html">
   Initialize analyses
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.4_mainmenu_preprocess_features.html">
   Preprocess features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.5_mainmenu_train_supervised_classifiers.html">
   Train supervised classifiers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.6_mainmenu_visualize_classifiers.html">
   Visualize classifiers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.7_mainmenu_display_training_results.html">
   Display training results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.8_mainmenu_generate_master_files.html">
   Generate master files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.9_mainmenu_OOCV_analysis.html">
   Out of Sample Cross-Validation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Examples
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="4.0_Example.html">
   Example
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Functions in development
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="3.10_mainmenu_investigate_sample_size.html">
   Investigate sample size (simulation)
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fdocs/3.2.02_paramtemp_preprocessing_pipeline.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/docs/3.2.02_paramtemp_preprocessing_pipeline.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#add-preprocessing-step">
   Add preprocessing step
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#enable-spatial-operations-using-spatial-op-wizard">
   Enable spatial operations using Spatial OP Wizard
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regress-out-nuisance-covariates-3-2-02-2-preprocessing-pipeline">
     Regress out nuisance covariates {#3.2.02.2_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apply-dimensionality-reduction-method-to-data">
     Apply dimensionality reduction method to data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#standardize-data-3-2-02-4-preprocessing-pipeline">
     Standardize data {#3.2.02.4_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scale-data-3-2-02-5-preprocessing-pipeline">
     Scale data {#3.2.02.5_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normalize-to-group-mean-3-2-02-6-preprocessing-pipeline">
     Normalize to group mean {#3.2.02.6_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normalize-to-unit-vector-3-2-02-7-preprocessing-pipeline">
     Normalize to unit vector {#3.2.02.7_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apply-binning-method-to-data-3-2-02-8-preprocessing-pipeline">
     Apply binning method to data {#3.2.02.8_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#impute-missing-values">
     Impute missing values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prune-non-informative-columns-from-data-matrix-3-2-02-9-preprocessing-pipeline">
     Prune non-informative columns from data matrix {#3.2.02.9_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#remove-group-level-differences-using-offset-correction-3-2-02-10-preprocessing-pipeline">
     Remove Group-level differences using offset correction {#3.2.02.10_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rank-weight-features">
     Rank / Weight features
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extract-variance-components-from-data-3-2-02-12-preprocessing-pipeline">
     Extract variance components from data {#3.2.02.12_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#measure-deviation-from-normative-data-3-2-02-13-deviation-normative">
     Measure deviation from normative data {#3.2.02.13_deviation_normative}
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Preprocessing pipeline</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#add-preprocessing-step">
   Add preprocessing step
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#enable-spatial-operations-using-spatial-op-wizard">
   Enable spatial operations using Spatial OP Wizard
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regress-out-nuisance-covariates-3-2-02-2-preprocessing-pipeline">
     Regress out nuisance covariates {#3.2.02.2_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apply-dimensionality-reduction-method-to-data">
     Apply dimensionality reduction method to data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#standardize-data-3-2-02-4-preprocessing-pipeline">
     Standardize data {#3.2.02.4_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scale-data-3-2-02-5-preprocessing-pipeline">
     Scale data {#3.2.02.5_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normalize-to-group-mean-3-2-02-6-preprocessing-pipeline">
     Normalize to group mean {#3.2.02.6_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normalize-to-unit-vector-3-2-02-7-preprocessing-pipeline">
     Normalize to unit vector {#3.2.02.7_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apply-binning-method-to-data-3-2-02-8-preprocessing-pipeline">
     Apply binning method to data {#3.2.02.8_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#impute-missing-values">
     Impute missing values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prune-non-informative-columns-from-data-matrix-3-2-02-9-preprocessing-pipeline">
     Prune non-informative columns from data matrix {#3.2.02.9_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#remove-group-level-differences-using-offset-correction-3-2-02-10-preprocessing-pipeline">
     Remove Group-level differences using offset correction {#3.2.02.10_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rank-weight-features">
     Rank / Weight features
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extract-variance-components-from-data-3-2-02-12-preprocessing-pipeline">
     Extract variance components from data {#3.2.02.12_preprocessing_pipeline}
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#measure-deviation-from-normative-data-3-2-02-13-deviation-normative">
     Measure deviation from normative data {#3.2.02.13_deviation_normative}
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="preprocessing-pipeline">
<span id="paramtemp-preprocessing-pipeline"></span><h1>Preprocessing pipeline<a class="headerlink" href="#preprocessing-pipeline" title="Permalink to this headline">#</a></h1>
<p>An important part of any machine learning analysis is how the data is prepared or ’preprocessed’ prior to its analysis with a classification algorithm. This is also known as ’feature extraction’ because the features are extracted from the existing data before analysis. NeuroMiner has a number of options to prepare data and can be tailored to a users specific data problem.</p>
<p>It’s important to note that NeuroMiner performs preprocessing steps within the cross-validation framework. This means that when the option to preprocess the data is selected, it preprocesses the training data and applies the ’learned’ preprocessing parameters to the CV1 and CV2 test data partitions.</p>
<p>When the preprocessing module is loaded the user will see the following:</p>
<figure class="align-default" id="fig-3-2-02-nm-preproc-menu">
<img alt="neurominer preprocessing pipeline menu" src="docs/Images/NM_preproc_menu_imig.png" />
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Importantly, the preprocessing steps that are selected will be outlined <strong>in order of processing</strong> under the heading <strong>CV-Preprocessing Sequence</strong>. The user can add steps using the add preprocessing step and can modify steps using the other straightforward options within this menu–e.g., the user can modify the settings of a preprocessing step or can change the order of the steps.</p>
</div>
<section id="add-preprocessing-step">
<span id="id1"></span><h2>Add preprocessing step<a class="headerlink" href="#add-preprocessing-step" title="Permalink to this headline">#</a></h2>
<p>This option is to add a preprocessing step to the ”Preprocessing sequence generator”. Once this is selected, the user can select the option that they would like from the following list:</p>
<figure class="align-default" id="fig-3-2-02-nm-preproc-add">
<img alt="neurominer preprocessing options" src="../_images/NM_preproc_add.png" />
</figure>
<p>If you are using volumetric neuroimaging data, then there will also be an option included at the top of the list to: 1 : Enable spatial operations using Spatial OP Wizard. This is a special module designed to optimize filtering and smoothing within the cross-validation process and is always performed before any other preprocessing steps across the entire dataset (i.e., on test and training data). Once the user has selected one of the steps, for example to perform a dimensionality reduction, they are then redirected to the main preprocessing menu that will add the processing step; for example:</p>
<figure class="align-default" id="fig-3-2-02-nm-preproc-pipe">
<img alt="neurominer preprocessing pipeline" src="../_images/NM_preproc_pipe.png" />
</figure>
<p>You can see that now there is a line stating CV-PREPROCESSING SEQUENCE containing ”Step 3: Dimensionality reduction”. The ”Preprocessing sequence generator” indicates that the step is selected for further operations as indicated 3/3 (one out of a total of three) steps and this selection is also highlighted by the arrow symbols (&gt;&gt;). If an option requires a suboption (e.g., <a class="reference internal" href="#dim-red"><span class="std std-ref">dimensionality reduction</span></a>) then the suboption will be listed underneath the parent option and this can be selected using the arrows as well.</p>
<p>As such, you can now perform other operations within this menu, including removing the selected preprocessing step, inserting another preprocessing step at the same location, replacing the current preprocessing step with another one, or modifying the current preprocessing step. If you have added a spatial preprocessing step, then this will appear above this menu because it is conducted prior to the preprocessing sequence across the entire dataset.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The preprocessing steps will be conducted in the order that they appear in the CV-PREPROCESSING SEQUENCE menu.</p>
</div>
</section>
<section id="enable-spatial-operations-using-spatial-op-wizard">
<span id="spatial-op"></span><h2>Enable spatial operations using Spatial OP Wizard<a class="headerlink" href="#enable-spatial-operations-using-spatial-op-wizard" title="Permalink to this headline">#</a></h2>
<p>If your active data modality is neuroimaging data, you should see the following option (see also {numref}(fig:3.2.02_nm_preproc_menu)):</p>
<p><strong>1 | Select spatial operation [ No filtering ]</strong></p>
<p>On turning on this mode, you will see the following menu:</p>
<figure class="align-default" id="fig-3-2-02-nm-preproc-filter-options">
<img alt="neurominer preprocessing spatial smoothing" src="../_images/NM_preproc_smoothing.png" />
</figure>
<p>Once an option is selected and the parameters have been defined, you will see a new field above the ”CV-PREPROCESSING SEQUENCE”  as follows:</p>
<figure class="align-default" id="fig-3-2-02-nm-preproc-pipe2">
<img alt="neurominer preprocessing pipeline2" src="../_images/NM_preproc_pipe2.png" />
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In NeuroMiner, it is designated as ”NON-CV PREPROCESSING” because it occurs across the entire dataset before preprocessing. However, as described below, it’s important to note that for smoothing the user can select a range of parameters and then the optimal combination of smoothing parameters, preprocessing settings, and training settings is found.</p>
</div>
<p>TILL HERE
<strong>2: Absolute difference filtering (6 neighbors)</strong></p>
<p>Absolute difference filtering is when the difference is computed between
the voxel and each of the 6 nearest neighbors surrounding it. Then the
value of the voxel is divided by the summed differences of the
neighbors.</p>
<p><strong>3: Cube variance filtering (27 neighbors)</strong></p>
<p>The variance of the 27 neighbors surrounding each voxel is calculated
and then the intensity of the target voxel is multiplied by the inverse
of this variance.</p>
<p><strong>4: Gaussian smoothing (=<span class="math notranslate nohighlight">\(&gt;\)</span>FWHM)</strong></p>
<p>This is regular Gaussian smoothing as used in most neuroimaging
toolboxes. The advantage of doing this in NeuroMiner is that you can
specify multiple different smoothing kernels (e.g., [6 8 10]) and then
these will be used as hyperparameters during optimisation. That is,
during learning in CV1 folds, the best combination of smoothing, other
preprocessing steps, and learning parameters will be determined and
applied to the held-out CV2 fold.</p>
<p><strong>5: Resampling (=<span class="math notranslate nohighlight">\(&gt;\)</span>Voxel size)</strong></p>
<p>This is regular voxel resampling as used in neuroimaging toolboxes. In
similarity to smoothing, the advantage of having it in NeuroMiner is
that you can optimise across different resampling parameters to find the
best combination of resampling, preprocessing, and training.</p>
<p>A critical point to remember in this mode is that if the user is
interested to observe the Z-score by using the Visualisation option, it
is essential for the user to use smooth data first. Applying a scaling
model to unsmoothed imaging data will likely result in extremely
high/low values in the test data due to non-overlap of tissue borders
between training and test cases.</p>
<section id="regress-out-nuisance-covariates-3-2-02-2-preprocessing-pipeline">
<h3>Regress out nuisance covariates {#3.2.02.2_preprocessing_pipeline}<a class="headerlink" href="#regress-out-nuisance-covariates-3-2-02-2-preprocessing-pipeline" title="Permalink to this headline">#</a></h3>
<p>If covariates have been entered when the data was entered into
NeuroMiner (see
<a class="reference external" href="#3.1.02_add_modality">[3.1.02_add_modality]</a>{reference-type=”ref”
reference=”3.1.02_add_modality”}), then you can apply the correction
using this step. This option is designed to remove the variance
associated with a nuisance variable (e.g., age, sex, study center) from
the data within each CV fold. When chosen it will reveal the following
options:</p>
<p>1 : Select method [Partial Correlations]<br />
2 : Select covariates [ age ]<br />
3 : Include intercept [ yes ]<br />
4 : Attenuate or Increase covariate effects [ attenuate ]<br />
5 : Use externally-computed beta coeficients [ no ]<br />
6 : Define subgroup of training cases for computing betas [ no ]</p>
<p>The first option gives you two possible methods to regress out the
covariates: Partial Correlations and ComBat. Partial correlations method
computes the coefficients that generates the chosen covariate effects in
the data with a general linear model. In addition to partial
correlations method, ComBat methos also scales the variance estimators.
It allows to seperate the label (eg. disease) effects and the covariate
effects in the data to some degree. For more detailed information about
ComBat method please check <a class="reference external" href="https://academic.oup.com/biostatistics/article/8/1/118/252073">Johnson et al.,
2007</a>).</p>
<p>Please be aware that ComBat does not provide you with an external
validation mode if new sites appear in the test data. For this case,
there is a function in NeuroMiner called nk_MultiCentIntensNorm2.m. This
function binarizes the interacting covariate and finds the bin with the
largest overlap (N) between sites. In this subgroup it computes partial
correlation coefficients or offsets and applies those to the entire
group to correct for site effects.</p>
<p>Selecting the second option will allow you to choose the covariates that
you want to control the data with:</p>
<p>(1) age<br />
(2) sex<br />
Select covariate(s) (enter expression - integer(s))<br />
Select covariate(s) (Default: 1 ) :</p>
<p>You select the covariate(s) by entering in either a single numeral
(e.g., 1) or many (e.g., 1:2) relating to the covariates that have been
previously entered. Once these are selected, the user will be returned
to the partial correlations setup menu.</p>
<p><strong>2: include intercept</strong>. The choice of whether to include an intercept
is based on your research question and relate to intercept inclusion in
any other use of regression.</p>
<p><strong>3: attenuate or increase covariate effects</strong>. Allows the user to
either attenuate or increase the effects of the covariate on the data
using regression.</p>
<p><strong>4: Use externally-computed beta coefficients</strong>. Allows the user to
enter beta coefficients from a regression that has been previously
calculated. This option only works if the dimensionality of the beta
coefficients exactly matches the dimensionality of the data. This means
that NM will crash if the dimensionality of the data is dynamically
changed during previous preprocessing steps.</p>
<p><strong>5: Define subgroup of training cases for computing betas</strong>. Option 5
allows the user to define a subgroup of training cases for computing
beta coefficients, which are then applied to the data (as discussed in
the supplementary material of <a class="reference external" href="https://www.ncbi.nlm.nih.gov/pubmed/25935725">Koutsouleris et al.,
2015</a> and <a class="reference external" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0022193">Dukart et al.,
2016</a>).
This function is useful when the user does not want to remove variance
that may interact with the target of interest. For example, brain volume
in schizophrenia interacts with age, therefore, the relationship between
age and brain volume can be modelled in the control participants only
and then these beta coefficients can be used in the schizophrenia sample
to remove the effects of age without removing the effects of illness.
When this option is selected, the user will see the following:</p>
<p>Compute beta(s) only from a specific subgroup (e.g. HC)?<br />
yes / no (Default: no)?<br />
Define index vector (ones = used / zeros = unused) for beta computation
:</p>
<p>Here, the user needs to enter in a MATLAB logical vector consisting of
TRUE and FALSE corresponding to the participants in the study; e.g., [0
0 0 0 1 1 1 1]. A FALSE value (i.e., a zero) indicates that they will
not be used in the calculation of the betas. A TRUE value (i.e., a 1)
indicates that they will be used in the calculation of the betas.
Logical vectors can be created from normal double vectors by simply
typing “logical(yourvector)” on the command line.</p>
</section>
<section id="apply-dimensionality-reduction-method-to-data">
<h3>Apply dimensionality reduction method to data<a class="headerlink" href="#apply-dimensionality-reduction-method-to-data" title="Permalink to this headline">#</a></h3>
<p>A common need in machine learning analysis is to reduce the
dimensionality of the data within the cross-validation framework (e.g.,
with structural neuroimages containing about 50,000 voxels). NeuroMiner
allows the user to do this using a number of different methods.</p>
<p>When the option to reduce dimensionality is selected, the user will be
shown the following menu:</p>
<p>1 : Principal Component Analysis (PCA)<br />
2 : Robust Principal Component Analysis<br />
3 : Non-negative Matrix Factorization<br />
4 : Orthogonal Projective NNMF (significantly slower but more robust
compared to NNMF) (currently visualisation mode is not integrated for
the algorithm)<br />
5 : Fast-than-fast NNMF (fastest NNMF through random prejections and
Nesterov iterations) (currently visualisation mode is not integrated for
the algorithm)<br />
6 : Partial Least Squares (PLS)<br />
7 : Sparse PCA<br />
8 : Probabilistic PCA (currently visualisation mode is not integrated
for the algorithm)<br />
9 : Factor analysis<br />
10 : Locality Preserving Projections<br />
11 : Linear Local Tangent Space Alignment (currently visualisation mode
is not integrated for the algorithm)<br />
12 : Large-Margin Nearest Neighbour (currently visualisation mode is not
integrated for the algorithm)<br />
13 : Linear Discriminant Analysis (low-D only) (currently visualisation
mode is not integrated for the algorithm)<br />
14 : Neighborhood Component Analysis<br />
15 : * General Discriminant Analysis (low-D only)<br />
16 : * Fast Maximum Variance Unfolding (currently not functional)<br />
17 : * Laplacian Eigenmaps (currently not functional)<br />
18 : * Local Linear Embedding (currently not functional)<br />
Each of these options will ask for parameters that are specific to the
type of dimensionality reduction being conducted and are included in the
respective sites for each technique listed below.</p>
<p>The main variable that can be changed across dimensionality reduction
types is the number of dimensions that are retained following reduction
(e.g., retaining 10 PCA components following dimensionality reduction of
neuroimaging data). The following example applies to PCA reduction
because this is the most common and we have found that it produces
robust results.</p>
<p>For PCA, NeuroMiner gives the option to select the number of dimensions
with the following options:</p>
<p>1 : Define extraction mode for PCA [ Energy range ]<br />
2 : Define extraction range [ 0.8 ]</p>
<p>These options allow the user to select how the components are extracted
and an extraction range based on this setting. When option 1 is
selected, you will see the following menu:</p>
<p>1 : Absolute number range [ 1 … n ] of eigenvectors<br />
2 : Percentage range [ 0 … 1 ] of max dimensionality<br />
3 : Energy range [ 0 … 1 ] of maximum decomposition</p>
<p><strong>Absolute number range [ 1 … n ] of eigenvectors</strong>. A whole number
of components that they would like to retain a priori.</p>
<p><strong>Percentage range [ 0 … 1 ] of max dimensionality</strong>. A percentage
of components to keep out of the total number of components.</p>
<p><strong>Energy range [ 0 … 1 ] of maximum decomposition</strong>. Retaining
components based on the percentage of the total amount of variance
explained (i.e., energy). For example, you might want to keep all
components that explain 80% of the variance of your data.</p>
<p>For each of these options, except PLS, NeuroMiner gives the user the
option to optimise the number of components that are selected during
cross-validation by specifying a range of values. For example, a range
of 20%, 40%, and 60% can be selected by entering [0.2 0.4 0.6].
NeuroMiner will then conduct all optimization procedures using these
percentages of retained variance–i.e., it will find the best PCA
reduction considering the other settings. If a range of values is
required, then it is recommended that an additional substep is
performed.</p>
<p><strong>Extracting Subspaces</strong></p>
<p>NeuroMiner as an additional option to first conduct the decomposition
and then to retain different component numbers for further analysis. You
do this by following the above procedure, but specifying that you want
to retain a singular value of components (e.g., for PCA, 100% of the
energy is recommended for the next step). Then you return to the
preprocessing menu and select the option to “Add a preprocessing
step”. In the list of steps, there will now be an option to “Extract
subspaces from reduced data projections” and the following menu will
appear:</p>
<p>1 : Define extraction mode for PCA [ Energy range ]<br />
2 : Define extraction range [ 1 ]</p>
<p>Using these functions you can then select the components that you want
to retain without having to run separate dimensionality reduction
analyses. For example, you can retain 100% of the energy in the first
step of a PCA to reduce the data dimensions and then retain a range of
subspaces of components in the second step; e.g., [0.2 0.4 0.6 0.8].
This dramatically increases processing time.</p>
<p>The following advanced dimensionality reduction techniques are also
offered by NeuroMiner, and the settings will change based on the
technique. We recommend to follow the links provided below to determine
the settings that are required or to evaluate the default settings
offered within NeuroMiner.</p>
<p><strong>Dimensionality Reduction Techniques</strong></p>
<ol class="simple">
<li><p>For all techniques see nk_PerfRedObj.m</p></li>
<li><p>Principal Component Analysis (PCA) – <a class="reference external" href="http://www.cad.zju.edu.cn/home/dengcai/Data/DimensionReduction.html">PCA by Deng
Cai</a></p></li>
<li><p>Robust Principal Component Analysis –
<a class="reference external" href="http://wis.kuleuven.be/stat/robust/LIBRA/LIBRA-home">LIBRA</a><br />
Note that RPCA is significantly slower than PCA but more accurate in
low N problems.</p></li>
<li><p>Non-negative Matrix Factorization – <a class="reference external" href="https://sites.google.com/site/nmftool/">Li and Ngom’s NMF
toolbox</a><br />
Note that NMF is significantly slower than PCA but more parsimonious
&amp; robust.</p></li>
<li><p>Partial Least Squares<br />
performs a single value decomposition (SVD; matlab built-in) on a
covariance matrix constructed from the entered features plus another
feature set to get latent variables. For example, if the primary
features are voxels from a brain and the secondary PLS feature set
are the labels it conduct SVD on the combined matrix (after
standardization). It then multiplies this unitary matrix (U) from
the SVD with the original standardized primary feature matrix in
order to sensitise the analysis to the combination of the features.
Other data can be used in place of the labels. The user has the
option to use sparse PLS based on the function “spls”.</p></li>
<li><p>Sparse PCA – [Please check preproc/spca.m; <a class="reference external" href="http://www.it.dtu.dk/projects/manifold/Papers/sparsepc.pdf">Zou et al.,
2004</a>]()<br />
This is a ‘statistical segmentation’ tool.</p></li>
<li><p>Simple Principal Component Analysis – <a class="reference external" href="https://lvdmaaten.github.io/drtoolbox/">MTDR; van der
Maaten</a></p></li>
<li><p>Probabilistic PCA – <a class="reference external" href="https://lvdmaaten.github.io/drtoolbox/">MTDR; van der
Maaten</a><br />
This is for low dimensional problems only, it requires
<span class="math notranslate nohighlight">\(&gt;\)</span><code class="docutils literal notranslate"><span class="pre">&lt;!--</span> <span class="pre">--&gt;</span></code>{=html}32GB RAM for high-D data.</p></li>
<li><p>Factor analysis – <a class="reference external" href="https://lvdmaaten.github.io/drtoolbox/">MTDR; van der
Maaten</a></p></li>
<li><p>Locality Preserving Projections – <a class="reference external" href="https://lvdmaaten.github.io/drtoolbox/">MTDR; van der
Maaten</a><br />
This is for low-dimensional problems only. Number of cases needs to
be <span class="math notranslate nohighlight">\(&gt;\)</span> no. features.</p></li>
<li><p>Linear Local Tangent Space Alignment – <a class="reference external" href="https://lvdmaaten.github.io/drtoolbox/">MTDR; van der
Maaten</a><br />
For low-D only, requires <span class="math notranslate nohighlight">\(&gt;\)</span><code class="docutils literal notranslate"><span class="pre">&lt;!--</span> <span class="pre">--&gt;</span></code>{=html}32 GB RAM for high-D
data.</p></li>
<li><p>Large-Margin Nearest Neighbour – <a class="reference external" href="https://lvdmaaten.github.io/drtoolbox/">MTDR; van der
Maaten</a></p></li>
<li><p>Deep Autoencoder – <a class="reference external" href="https://lvdmaaten.github.io/drtoolbox/">MTDR; van der
Maaten</a><br />
Low dimensional data only.</p></li>
<li><p>Neighborhood Component Analysis <a class="reference external" href="https://lvdmaaten.github.io/drtoolbox/">MTDR; van der
Maaten</a><br />
Low dimensional data only.</p></li>
</ol>
<p>* For implementation of all functions, please see nk_PerfRedObj.m and
files in the preproc directory.</p>
</section>
<section id="standardize-data-3-2-02-4-preprocessing-pipeline">
<h3>Standardize data {#3.2.02.4_preprocessing_pipeline}<a class="headerlink" href="#standardize-data-3-2-02-4-preprocessing-pipeline" title="Permalink to this headline">#</a></h3>
<p>The standardization, scaling, and/or normalisation of data is an
important part of most machine learning. NeuroMiner allows contains
different methods to perform these functions within the preprocessing
modules, which are conducted within each inner cross-validation fold.</p>
<p>Standardization is conducted on each of the features that have been
entered to NeuroMiner across observations.</p>
<p>When option 3 is selected, the user will see the following:</p>
<p>1 : Select standardization method<br />
2 : Compute standardization using a subgroup of cases [ no ]<br />
3 : Apply standardization model to a subgroup of cases [ no ]<br />
4 : Winsorize data (clamping of outliers) [ no ]<br />
5 : Zero-out completely non-finite features [ yes ]</p>
<p><strong>1 : Select standardization method</strong></p>
<p>Select from the following standardization options:</p>
<p>1 | standardization using the median<br />
2 | standardization using the mean<br />
3 | mean-centering<br />
4 | l1-median centering<br />
5 | qn-standardization<br />
6 | sn-standardization</p>
<p>Option 1 calculates the Z-score using the median (i.e., (score -median)
/ standard deviation) and option 2 calculates the Z-score using the
mean. Option 3 mean centers all features (i.e., subtraction of the mean
for the feature across all scores). Option 4 l1-median centering
calculates the multivariate L1-median and then takes the derivatives
from this as features (see nk_PerfStandardizeObj.m; Hossjer and Crous
(1995) Generalizaing univariate signed rank statistics for testing and
estimating a multivariate location parameter, Non-parametric statistics,
4, 293-308). Options 5 and 6 are alternatives to the median absolute
deviation for standardizing multivariate data (see Rousseeuw and Croux
(1993). Alternatives to the median absolute deviation, J. Am. Statist.
Assoc., 88).</p>
<p><strong>2 : Compute standardization using a subgroup of cases [ no ]</strong></p>
<p>User can compute the standardization using a subgroup of cases.</p>
<p><strong>3 : Apply standardization model to a subgroup of cases [ no ]</strong></p>
<p>User can apply the standardization model to a subgroup of cases.</p>
<p><strong>4 : Winsorize data (clamping of outliers) [ no ]</strong></p>
<p>The user also has the option to Winsorize or ‘censor’ the data. This
technique was introduced to account for outliers. After standardization,
the elements which are outside of a user-defined range (e.g., 4 standard
deviations) are set to their closest percentile. For example, data above
the 95th percentile is set to the value of the 95th percentile. The
feature is then re-centered using the new censored values.</p>
<p><strong>5 : Zero-out completely non-finite features [ yes ]</strong></p>
<p>If there are entries in the user’s feature data that are non-numeric
(i.e., “NaN” or “not a number” elements), then these will not be
considered during the calculation of the mean and standard deviation
(NeuroMiner uses the nanmean function). However, after the data has been
standardized, these values will be added back to the feature matrix. As
such, NeuroMiner gives the option to change these to zero in step four.
It is important to note that once the data has been standardized, zero
values reflect the mean of the feature and are thus a form of mean
imputation. If the user wants to use other imputation options, they can
select ‘no’ to this option and then impute the data using the
preprocessing imputation option.</p>
</section>
<section id="scale-data-3-2-02-5-preprocessing-pipeline">
<h3>Scale data {#3.2.02.5_preprocessing_pipeline}<a class="headerlink" href="#scale-data-3-2-02-5-preprocessing-pipeline" title="Permalink to this headline">#</a></h3>
<p>Another option to put the data into the same space is to scale it
between two values. The user will see the following menu when they
select this option:</p>
<p>1: Scale across features or each feature independently [each feature
independently]<br />
2: Define scale range [0,1]<br />
3: Zero-out completely non-finite features [yes]</p>
<p>Using option 1, scaling can be conducted for “each feature
independently” or “across the entire matrix”. If each feature
independently is selected, then the data will be scaled within the
desired range for each feature (i.e., each voxel or each questionnaire).
If the entire matrix is selected then each value in the matrix will be
scaled according to the range of all the data (e.g., all voxels or all
questionnaires).</p>
<p>The user then has the option to define the scale range between 0 and 1
or between -1 to 1. These two options are provided because some machine
learning algorithms require the data to be scaled differently; e.g., the
liblinear options require the data to be scaled between -1 to 1.</p>
<p>Non-numeric values are not taken into account during scaling and are
added back to the matrix after scaling. The third option gives the user
the ability to either change these values to zero following scaling by
selecting “yes” for the third option to “zero-out completely
non-finite features”. It is critical to note that in this case the
values will be considered to be the absolute minimum of the scale for
future analyses (e.g., if IQ is scaled, then these individuals will have
an IQ value of 0). For questionnaire data, it is recommended that this
option is changed to “no”, and instead imputation is performed after
scaling for the NaN elements. Alternatively, for imaging data, it is
recommended that NaN values are excluded by using a brainmask during
data input.</p>
</section>
<section id="normalize-to-group-mean-3-2-02-6-preprocessing-pipeline">
<h3>Normalize to group mean {#3.2.02.6_preprocessing_pipeline}<a class="headerlink" href="#normalize-to-group-mean-3-2-02-6-preprocessing-pipeline" title="Permalink to this headline">#</a></h3>
<p>This option is only available when a group has been entered as a
covariate at the start of the analysis (i.e., dummy coded vector
containing ones for the group and zeros for the other participants). The
option will then normalise all subjects to the mean of the specified
group.</p>
</section>
<section id="normalize-to-unit-vector-3-2-02-7-preprocessing-pipeline">
<h3>Normalize to unit vector {#3.2.02.7_preprocessing_pipeline}<a class="headerlink" href="#normalize-to-unit-vector-3-2-02-7-preprocessing-pipeline" title="Permalink to this headline">#</a></h3>
<p>This option takes the mean of the data, then subtracts this from each of
the scores within the data. It then calculates the norm (L1 or L2) of
this result. It then mean-centers the original data and divides each
score by the normalised scores.</p>
</section>
<section id="apply-binning-method-to-data-3-2-02-8-preprocessing-pipeline">
<h3>Apply binning method to data {#3.2.02.8_preprocessing_pipeline}<a class="headerlink" href="#apply-binning-method-to-data-3-2-02-8-preprocessing-pipeline" title="Permalink to this headline">#</a></h3>
<p>Binning is a technique that can be used to control for variance in data
(<a class="reference external" href="https://en.wikipedia.org/wiki/Data_binning">wiki</a>). For example, a
histogram is an example of data binning. A form of it is discretization
of continuous features
(<a class="reference external" href="https://en.wikipedia.org/wiki/Discretization_of_continuous_features">wiki</a>)
and this is offered in NeuroMiner as the default option when binning is
selected as follows:</p>
<p>1 : Select binning method [ discretize ]<br />
2 : Define binning start value [ 0 ]<br />
3 : Define binning stepping [ 0.5 ]<br />
4 : Define binning stop value [ 4 ]</p>
<p>Discretization basically just forms bins based on the standard
deviation. You first establish a vector using a start value, a stepping
value, and a stop value called alpha (e.g., the default setting is [0
0.5 1 1.5 2 2.5 3 3.5 4]). It then discretizes each feature by the mean
+/- alpha*std. For example, for the first value of alpha, it finds the
feature values that are above and below the mean and gives them a new
value (i.e., 1 or -1). Then in the next step it finds the values that
are above and below the mean +/- half of the standard deviation and
gives them a new value (i.e., 2 or -2). In the next step it finds values
that are above and below one standard deviation from the mean and gives
them a new value (i.e., 3 or -3). And so on. In this way, bins are
created with a width that is determined by the standard deviation. It is
important to note that this function will perform Winsorization (i.e.,
clamping) because values the stop value of alpha (e.g., above 4 standard
deviations above or below the mean) will be given the final value.</p>
<p>Manually determining the bins is a good way to discretize data if you
have a firm idea about the problem, but if you do not then it is useful
to find an optimal bin width for a feature by taking into account the
information content of the data. This can be done in NeuroMiner by
changing the “1: Select binning method” to “Unsupervised
entropy-based symbolization”. It tries to find an optimal bin width for
a feature by taking into account the entropy. It will do this within the
range that you specify in the following settings:</p>
<p>1 : Select binning method [ symbolize ]<br />
2 : Define minimum bin count [ 3 ]<br />
3 : Define maximum bin count [ 25 ]<br />
4 : Define sequence length [ 4 ]<br />
5 : Define width of data range in number of STDs from mean [ 3 ]</p>
</section>
<section id="impute-missing-values">
<h3>Impute missing values<a class="headerlink" href="#impute-missing-values" title="Permalink to this headline">#</a></h3>
<p>The machine learning algorithms included in NeuroMiner are not able to
process missing values (represented in MATLAB as “Not a Number” (NaN)
entries). This means that they have to be either removed using the
“Prune non-informative columns” feature described below, or they need
to be imputed using this module.</p>
<p>NeuroMiner performs imputation using either single-subject median
replacement, feature-wise mean replacement, or multivariate
distance-based nearest-neighbor median imputation. When this feature is
selected it will reveal the following menu:</p>
<p>1 : Define imputation method [ kNN imputation (EUCLIDEAN) ]<br />
2 : Select features for imputation [ All features ]<br />
3 : Define number of nearest neighbors [ 7 nearest neighbors ]</p>
<p><strong>1: Define imputation method</strong></p>
<p>You have to first define the imputation method using the following menu:</p>
<p>1 : Median of non-NaN values in given case<br />
2 : Mean of non-NaN values in given feature<br />
3 : MANHATTAN distance-based nearest-neighbor search<br />
4 : EUCLIDEAN distance-based nearest-neighbor search<br />
5 : SEUCLIDEAN distance-based nearest-neighbor search<br />
6 : COSINE similarity-based nearest-neighbor search<br />
7 : HAMMING distance-based nearest-neighbor search<br />
8 : JACCARD distance-based nearest-neighbor search<br />
9 : Nearest-neighbor imputation using hybrid method</p>
<p>For median or mean replacement, values are imputed based on the data
either across features for one subject or across subjects within the
feature. For multivariate nearest-neighbor imputation, for each case
with NaN values, a multivariate statistical technique is conducted to
identify a number of similar (i.e., nearest-neighbor) cases from all the
subjects that are available. For example, the Euclidean distance could
be used to identify the 7 nearest-neighbor subjects that are close to
the subject with the NaN value. Once these similar cases are identified
in the multivariate space, then NeuroMiner will take the median of their
values of the feature with the missing NaN case and impute it into the
NaN field. A number of distance metrics can be used to represent the
cases in multivariate space and to determine the nearest-neighbors based
on the type of data you have (e.g., Manhattan, Euclidean, Seuclidean,
Cosine, Hamming, or Jaccard). For example, the Euclidean distance could
be used for data with a continuous measurement scale or the Hamming
could be used for binary nominal data (i.e., 0 or 1) data (see Box:
Nominal Data). There is also the option to use a hybrid approach that
accounts for both continuous/ordinal and nominal data. It’s important to
note that all options except for the Manhattan and Euclidean distances
require the Statistics and Machine Learning Toolbox of MATLAB.</p>
<p>1: Median of non-NaN values in given case<br />
This function checks NaN values for each subject. When it finds a NaN
value for a subject, it imputes the median of the non-NaN values across
the other features for that subject. Therefore, this would make no sense
at all if it was done across features that weren’t equivalent in scale
or from the same scale (e.g., a questionnaire). As such, it could be
used in combination with the option to “2: Select features for
imputation” described below.</p>
<p>2: Mean of non-NaN values in given feature<br />
This function cycles through each feature and if it finds a NaN value
then it imputes the mean of non-NaN values.</p>
<p>3: MANHATTAN distance-based nearest-neighbor search<br />
This function determines the Manhattan distance between cases and then
selects nearest-neighbors. It uses the ‘distance’ function from the
Large Margins Nearest-Neighbors (LMNN) toolbox version 2.5
<a class="reference external" href="http://www.cs.cornell.edu/~kilian/code/lmnn/lmnn.html">(link)</a>. You
must scale, unit-normalize or standardize the data first, otherwise the
distance measure will be dominated by high-variance features.</p>
<p>4: EUCLIDEAN distance-based nearest-neighbor search<br />
Determines the Euclidean distance between cases and then selects
nearest-neighbors. It uses the ‘distance2’ function from the Large
Margins Nearest-Neighbors (LMNN) toolbox version 2.5
<a class="reference external" href="http://www.cs.cornell.edu/~kilian/code/lmnn/lmnn.html">(link)</a>. You
must scale, unit-normalize or standardize the data first, otherwise the
distance measure will be dominated by high-variance features.</p>
<p>5: SEUCLIDEAN distance-based nearest-neighbor search<br />
The Seuclidean distance between cases is measured using the pdist2
function that is built into the Statistics and Machine Learning Toolbox
of MATLAB.</p>
<p>6 : COSINE similarity-based nearest-neighbor search<br />
The Cosine distance between cases is measured using the pdist2 function
that is built into the Statistics and Machine Learning Toolbox of
MATLAB.</p>
<p>7 : HAMMING distance-based nearest-neighbor search<br />
The Hamming distance between cases can be used for nominal data (e.g.,
0/1 data) and is measured using the pdist2 function that is built into
the Statistics and Machine Learning Toolbox of MATLAB.</p>
<p>8 : JACCARD distance-based nearest-neighbor search<br />
The Jaccard distance between cases is measured using the pdist2 function
that is built into the Statistics and Machine Learning Toolbox of
MATLAB.</p>
<p>9 : Nearest-neighbor imputation using hybrid method<br />
This option combines techniques that can be used for binary nominal data
(i.e., 0/1) and ordinal/continuous data. You must define a method for
nominal features (e.g., Hamming or Jaccard) and then define a method for
ordinal/continuous features (e.g., Euclidean or Cosine). The maximum
number of unique values for nominal feature imputation must also be
inputted.</p>
<p><strong>2: Select features for imputation</strong><br />
The imputation procedures can be applied to the entire dataset or to
subsets of the data. This is useful when there are different data
domains (e.g., clinical and cognitive data) or even single
questionnaires (e.g., the PANSS) that you want to restrict the
imputation to. A MATLAB logical vector (i.e., consisting of TRUE and
FALSE fields that are depicted as [0 0 0 1 1 0 0 etc.]; you can
convert a binary vector to a logical simply by typing
logical(yourvector) on the command line) must be provided to select a
subset. If you want to impute multiple subsets then you must add
separate imputation routines.</p>
<p><strong>3: Define number of nearest-neighbors</strong><br />
If you’re using the multivariate nearest-neighbors approach then you
must define the number of nearest neighbors (e.g., subjects) to use for
the calculation of the median value. The default is 7.</p>
</section>
<section id="prune-non-informative-columns-from-data-matrix-3-2-02-9-preprocessing-pipeline">
<h3>Prune non-informative columns from data matrix {#3.2.02.9_preprocessing_pipeline}<a class="headerlink" href="#prune-non-informative-columns-from-data-matrix-3-2-02-9-preprocessing-pipeline" title="Permalink to this headline">#</a></h3>
<p>Data pruning is the removal of examples or features that will not work
with machine learning algorithms or are redundant. In NeuroMiner, the
machine learning algorithms that are used cannot work with missing
values (i.e., “Not a Number” (NaN) entries in MATLAB) or with infinite
values (i.e., “Inf” in MATLAB), which can occur due to previous
processing steps. If the values of a feature do not change between
participants (i.e., there is zero variance), then it is redundant to
include the feature in the analysis as well. To correct for these
problems, NeuroMiner offers the following options:</p>
<p>1 : Prune zero features [ no ]<br />
2 : Prune features with NaNs [ yes ]<br />
3 : Prune features with Infs [ yes ]<br />
4 : Prune features with single-value percentage over cutoff [ yes, 5 ]</p>
<p>The first option gives the user the ability to remove variables with no
variance within a fold (i.e., if all individuals, or ‘examples’ within a
cross-validation fold have exactly the same value for a feature).</p>
<p>The second option will remove features within a fold that have any NaN
(‘Not a Number’) values. Similarly, the third option will allow users to
remove features with any Infs (Infinity) values.</p>
<p>It is important to note that even if there is one subject with one NaN
or Inf within the fold NeuroMiner will remove the feature. The amount of
removed features will be difficult to quantify. For matrix data, check
the NaNs and potentially filter then prior to the entry into NeuroMiner
(see
<a class="reference external" href="#fig:matrix_inspector">[fig:matrix_inspector]</a>{reference-type=”ref”
reference=”fig:matrix_inspector”}).</p>
<p>The fourth option is used in situations where there are a large number
of subjects with the same value and a few with different values. For
example, in neuroimaging due to registration inaccuracies most of the
subjects may have a voxel value of zero in a location close to the gray
matter border and a few might have non-zero numbers. In these
circumstances, you might want to exclude the voxels where there is no
variance in, for example, 90% of the sample (e.g., where 90% of
individuals have a zero value). You can specify a percentage here to do
this.</p>
</section>
<section id="remove-group-level-differences-using-offset-correction-3-2-02-10-preprocessing-pipeline">
<h3>Remove Group-level differences using offset correction {#3.2.02.10_preprocessing_pipeline}<a class="headerlink" href="#remove-group-level-differences-using-offset-correction-3-2-02-10-preprocessing-pipeline" title="Permalink to this headline">#</a></h3>
<p>This function was introduced primarily to control for scanner
differences in structural neuroimaging data. In the case of two centers,
for each feature, the mean of both center A and center B is calculated.
The mean difference between these is then calculated (A-B=Y) and then
subtracted from each of the data points in A (i.e., Ai - Y). This value
is stored for further analysis. Internal empirical tests indicate that
this basic method can control for major site differences.</p>
</section>
<section id="rank-weight-features">
<h3>Rank / Weight features<a class="headerlink" href="#rank-weight-features" title="Permalink to this headline">#</a></h3>
<p>Selecting or ranking features is an important part of machine learning
and can be achieved with <strong>filters</strong> and <strong>wrappers</strong>. For a discussion
of why these methods are important and for all definitions see <a class="reference external" href="http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf">Guyon et
al., 2003</a>.
Wrappers will be discussed in section
<a class="reference external" href="#3.2.05_ensemble_generation_strategies">[3.2.05_ensemble_generation_strategies]</a>{reference-type=”ref”
reference=”3.2.05_ensemble_generation_strategies”} of the manual,
whereas filters are outlined here and also in section
<a class="reference external" href="#3.2.05_ensemble_generation_strategies">[3.2.05_ensemble_generation_strategies]</a>{reference-type=”ref”
reference=”3.2.05_ensemble_generation_strategies”}.</p>
<p>Filter methods broadly involve the weighting and ranking of variables,
and optionally the selection of top performing variables. This can be
conducted either prior to the evaluation of the model during
pre-processing or as part of model optimisation (discussed in section
<a class="reference external" href="#3.2.05_ensemble_generation_strategies">[3.2.05_ensemble_generation_strategies]</a>{reference-type=”ref”
reference=”3.2.05_ensemble_generation_strategies”}). This option allows
the user to weight variables in the training folds based on some
criteria (e.g., Pearson’s correlation coefficient with the target
variable) and then rank them in a specified order (e.g., descending
order from most to least correlated). When this option is selected, the
following will be displayed:</p>
<p>1: Choose algorithm and specify its parameters [Pearson’s]<br />
2: Define target labels [ NM target label ]<br />
3: Up- or down-weight predictive features [ up-weight features ]</p>
<p><strong>1: Choose algorithm and specify its parameters</strong><br />
The user here has the option to choose from among a number of methods
that will weight the relationship between the filter target and the
feature. These are:</p>
<p>1: Pearson / Spearman correlation<br />
Standard MATLAB functions for Pearson’s (corrcoef).</p>
<p>2: Spearman correlation<br />
Standard MATLAB function for Spearman’s (corr).</p>
<p>3: G-flip (Greedy Feature Flip)<br />
See <a class="reference external" href="http://www.cs.huji.ac.il/labs/learning/Papers/giladbachrachnavottishby04b.pdf">Gilad-Bachrach et al.
(2004)</a>
and
<a class="reference external" href="http://www.cs.huji.ac.il/labs/learning/code/feature_selection.bak/">site</a></p>
<p>4: Simba<br />
See <a class="reference external" href="http://www.cs.huji.ac.il/labs/learning/Papers/giladbachrachnavottishby04b.pdf">Gilad-Bachrach et al.
(2004)</a>
and
<a class="reference external" href="http://www.cs.huji.ac.il/labs/learning/code/feature_selection.bak/">site</a></p>
<p>5: IMRelief<br />
This is an implementation of the Yijun Sun IMRelief function called
“IMRelief Sigmoid FastImplentation” (see <a class="reference external" href="http://plaza.ufl.edu/sunyijun/Paper/ICML06_1.pdf">Sun &amp; Li,
2007</a>).</p>
<p>6: Relief for classification<br />
This is a version of the built-in MATLAB ReliefF algorithm
<a class="reference external" href="https://de.mathworks.com/help/stats/relieff.html">(link)</a>.</p>
<p>7: Linear SVC (LIBSVM)<br />
This weights the features by conducting a linear SVM using each
independent feature, as implemented in LIBSVM <a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">(Chang &amp;
Lin)</a>.</p>
<p>8: Linear SVC (LIBLINEAR)<br />
Weights the features based on using LIBLINEAR toolbox
<a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">(link)</a></p>
<p>9: F-Score<br />
Weights the features based on conducting an ANOVA using standard MATLAB
functions on the differences between the groups when problem is
classification.</p>
<p>10: AUC<br />
Calculates an area under the curve (AUC) for binary classifications
based on the code of Chih-Jen Lin and then ranks the features based on
this.</p>
<p>11: FEAST<br />
Implementation of the FEAST toolbox for feature selection
<a class="reference external" href="http://www.cs.man.ac.uk/~gbrown/fstoolbox/">(site)</a>.</p>
<p>12: ANOVA<br />
Standard implementation of ANOVA.</p>
<p>13: PLS<br />
Performs a Partial Least Squares analysis to rank the features.</p>
<p>14: External ranking<br />
This option uses a user-defined weighting template that was entered
during data input (see
<a class="reference external" href="#mainmenu_3.1_input_data">[mainmenu_3.1_input_data]</a>{reference-type=”ref”
reference=”mainmenu_3.1_input_data”}), to use an external weighting map
from a file, or to read-in a weighting vector using the MATLAB
workspace.</p>
<p><strong>2: Define target labels</strong><br />
Here the user defines the target label that the filter will be applied
to using the original data; e.g., this could be your group labels if you
have a classification problem or your questionnaire item if you’re using
regression. The default setting in NeuroMiner is to use your target
variable, but this van be changed by first selecting the menu item and
then entering “D” to define a new target label. A new menu will appear
and you can select to enter either categorical or continuous data. You
can then enter a MATLAB vector that defines either categorical labels
(e.g., [0 0 0 0 1 1 1 1]) or continuous labels (e.g., [15 26 19 33
22]). These variables should be stored in your MATLAB workspace.</p>
<p>When a continuous variable is selected, the user will then be asked
whether they want to weight feature using only one specific subgroup.
This means that the feature will only be weighted only based on the
relationship between the feature and the filter target in one subgroup
(e.g., control participants).</p>
<p><strong>3: Up- or down-weight predictive features</strong><br />
This option allows the user to either upweight or downweight the
features based on the relationship between the variable and the target.
For example, if you want to increase the importance of the features
based on the target you would upweight the features. If you want to
reduce the importance of the features based on some other variable
(e.g., age if it is a nuisance variable) you could select the variable
as the feature selection target and then choose the downweight option.
For more information about why you would want to do this see references
above.</p>
</section>
<section id="extract-variance-components-from-data-3-2-02-12-preprocessing-pipeline">
<h3>Extract variance components from data {#3.2.02.12_preprocessing_pipeline}<a class="headerlink" href="#extract-variance-components-from-data-3-2-02-12-preprocessing-pipeline" title="Permalink to this headline">#</a></h3>
<p>This function takes a source matrix and performs a PCA. It then
determines correlations between the eigenvariates from the PCA
reduction, and those above or below a specified threshold are
identified. It then projects the target matrix to the source matrix PCA
space, and then back-reconstructs this to the original input space
without the identified PCA components (i.e., it removes the variance
associated with those components).</p>
</section>
<section id="measure-deviation-from-normative-data-3-2-02-13-deviation-normative">
<h3>Measure deviation from normative data {#3.2.02.13_deviation_normative}<a class="headerlink" href="#measure-deviation-from-normative-data-3-2-02-13-deviation-normative" title="Permalink to this headline">#</a></h3>
<p>This function calculates a PLS and then determines how much the data
deviates from the model that is produced. The hypothesis here is that
the PLS model will represent a normative variation between the primary
features (e.g., the brain) and the second feature matrix (e.g., the
labels or another variable set). By calculating the deviation from this
model, it is hypothesised that normative deviation will be modeled and
then included in further processing steps.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="3.2.01_paramtemp_cv_settings.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Cross-validation settings</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3.2.03_paramtemp_classification_algorithm.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Classification algorithm</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Neurodiagnostics Lab LMU Munich<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>