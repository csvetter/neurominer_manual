(3.7_mainmenu_display_training_results)=
# Display training results

Once initialization, preprocessing, and model training has been performed, the results can then be displayed using this option. The results display interface can be understood in three main divisions:
1. the accuracy and model performance (initial screen)
2. the model performance across the selected hyperparameters
3. results concerning the individual features in the analysis

## Accuracy and model performance
See {numref}(fig:Display_ClassPlot): The performance of a model can be fundamentally described by the degree to which the predictions match the target scores (e.g., how many people are classified correctly or how closely the features and target are related in a regression). In NeuroMiner, this is represented in multiple ways that each address a different aspect of the relationship between subjects, predictions, and targets.

```{figure} Images/Display_ClassPlot.percentage
---
alt: Neurominer result viewer, classification plot
name: fig:Display_ClassPlot
---
Result viewer option 1: Classification plot
```

## Model comparisons
See {numref}(fig:Display_Statistical_Comp): When there is more than one analysis, statistical comparisons can be conducted between the model performances using a variety of statistics using the ”Statistical Comparisons” option. The results of these comparisons are output in either excel spreadsheet files (i.e., xlsx) or in text files (i.e., csv) for Linux and Mac. The menu is quite intuitive (i.e., as below) where you select the analyses you want to compare on the right, you press the ”Save As” button to choose a name, and then you select the ”Compare” button to generate the files. The files will be generated in your working directory.

```{figure} Images/Display_StatisticalComp.png
---
alt: Neurominer result viewer, statistical comparisons
name: fig:Display_Statistical_Comp
---
Within the main results display, the user can go to the ”Comparisons” menu item and then select the option to calculate statistical differences between separate models
```

See {numref}(fig:Display_Visual_Comp): The user can also perform ”Visual Comparisons”. The user needs to select some performance measures that they want to compare visually in the top-half of the pop-up box using the ”Select” column tick-boxes, then select the analyses in the lower-half of the pop-up box. Then simply press ”Visualize”. If the bar plots are not good then the user can change it to ”Line plots” and then press ”Visualize” again.

```{figure} Images/Display_Visual_Comp.png
---
alt: Neurominer result viewer, visual comparisons
name: fig:Display_Visual_Comp
---
Within the main results display, the user can go to the ”Comparisons” menu item and then select the option to display visual comparisons between separate models
```

## Classification performance
See {numref}(fig:Display_CVperf): NeuroMiner was built to work with hyperparameter optimization within a nested-cross validation framework as a standard. In this plot, you will see the inner-cycle, CV1, mean test performance plotted in blue and then the outer cycle, CV2, mean test performance plotted in red. Each point represents a hyperparameter combination. This gives an indication of how the hyperparameter combinations are influencing the test performance. On the right, the display also shows heatmaps that show how the performance in the CV1/CV2 folds changes over the permutations.


```{figure} Images/Display_CVperf.png
---
alt: Neurominer result viewer, cv performance plot
name: fig:Display_CVperf
---
Result viewer option 2: Cross-validation performance
```


## Generalization error
See {numref}(fig:Display_GenError): Another way to look at CV2 test performance is with the generalization error, which is calculated in NeuroMiner as the CV1-test performance minus the CV2 test performance.

```{figure} Images/Display_GenError.png
---
alt: Neurominer result viewer, generalization error
name: fig:Display_GenError
---
Result viewer option 3: Generalization error
```

## Model complexity
See {numref}(fig:Display_Complexity). In NeuroMiner, complexity is defined as the percentage of support vectors divided by the number of subjects in the tested fold. This feature is useful to determine how varying parameter combinations may affect the fit of the model.

```{figure} Images/Display_Complexity.png
---
alt: Neurominer result viewer, complexity
name: fig:Display_Complexity
---
Result viewer option 4: Complexity
```

## Ensemble diversity
See {numref}(fig:Display_EnsembleDiversity): This is a measure of Shannon entropy across CV1 models for each CV2 partition. A higher entropy means higher disagreement between the CV1 models. This measure is useful because increased entropy will often result in a better fitting model at the CV2 level. It is useful to note that entropy can be maximized as a performance criterion in addition to accuracy.


```{figure} Images/Display_EnsembleDiv.png
---
alt: Neurominer result viewer, ensemble diversity
name: fig:Display_EnsembleDiv
---
Result viewer option 5: Model selection
```


## Model selection frequency
See {numref}(fig:Display_ModelSelection): Within an ensemble framework, the models that are selected within the inner, CV1, cycle are then applied to the outer test data. For example, if it is a 10x10 CV1 framework with a winner-takes-all method of model selection, then 100 models will be applied to the held-out CV2 test fold. This plot displays the percentage of times that each parameter combination was selected.

```{figure} Images/Display_ModelSelection.png
---
alt: Neurominer result viewer, model selection
name: fig:Display_ModelSelection
---
Result viewer option 6: Model selection frequency
```

#### Multi-group results
See {numref}(fig:Display_MultiGroup): If a multi-group analysis is conducted then the results display will change to accommodate the groups.

```{figure} Images/Display_MultiGroup.png
---
alt: Neurominer result viewer, model selection
name: fig:Display_MultiGroup
---
Result viewer option 7: Multi-group display
```
